{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PureNumpyConvNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSyNCV+1TFC6PveCDsaRl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RihaChri/PureNumpyConvNet/blob/main/PureNumpyConvNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPK1eXpl9Gkt",
        "outputId": "8c936534-5bbb-46bd-e36e-c0b0fe62ab38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mode = max\n",
            "mean of dA =  0.14571390272918056\n",
            "dA_prev[1,1] =  [[ 0.          0.        ]\n",
            " [ 5.05844394 -1.68282702]\n",
            " [ 0.          0.        ]]\n",
            "\n",
            "mode = average\n",
            "mean of dA =  0.14571390272918056\n",
            "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
            " [ 1.26461098 -0.25749373]\n",
            " [ 1.17975636 -0.53624893]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "np.random.seed(1)\n",
        "\n",
        "def zero_pad(X, pad):\n",
        "    #X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    #pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "    #X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
        "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)),'constant',constant_values = 0)\n",
        "    return X_pad\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    #a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    #W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    #b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "    #Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    s = a_slice_prev * W\n",
        "    Z = np.sum(s)\n",
        "    Z = Z+b\n",
        "    return Z\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    #A_prev -- output activations of the previous layer, \n",
        "    #    numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    #W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    #b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    #hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "    #Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    #cache -- cache of values needed for the conv_backward() function\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    stride = hparameters['stride']\n",
        "    pad = hparameters['pad']\n",
        "    n_H = int((n_H_prev-f+2*pad)/stride)+1\n",
        "    n_W = int((n_W_prev-f+2*pad)/stride)+1\n",
        "    Z = np.zeros((m,n_H,n_W,n_C))\n",
        "    A_prev_pad = zero_pad(A_prev,pad)\n",
        "    \n",
        "    for i in range(m):                              # loop over the batch of training examples\n",
        "        a_prev_pad = A_prev_pad[i]                # Select ith training example's padded activation\n",
        "        for h in range(n_H):                          # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):                      # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):                  # loop over channels (= #filters) of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    # Note: there is a stride, therefore it is not wise to assign vert_start a value of h and it is the same with horiz_start\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f\n",
        "                    \n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
        "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "      \n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
        "                    Z[i, h, w, c] = np.sum(a_slice_prev * W[:, :, :, c]) + float(b[:, :, :, c])\n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
        "\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
        "    \n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "\n",
        "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
        "    #A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    #hparameters -- python dictionary containing \"f\" and \"stride\"\n",
        "    #mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "    #A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
        "    #cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    f = hparameters[\"f\"]\n",
        "    stride = hparameters[\"stride\"]\n",
        "    n_H = int(1 + (n_H_prev - f) / stride)\n",
        "    n_W = int(1 + (n_W_prev - f) / stride)\n",
        "    n_C = n_C_prev\n",
        "    A = np.zeros((m, n_H, n_W, n_C))              \n",
        "    \n",
        "    for i in range(m):                          # loop over the training examples\n",
        "        for h in range(n_H):                    # loop on the vertical axis of the output volume\n",
        "            for w in range(n_W):                # loop on the horizontal axis of the output volume\n",
        "                for c in range (n_C):           # loop over the channels of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
        "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "                    \n",
        "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
        "                    if mode == \"max\":\n",
        "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
        "                    elif mode == \"average\":\n",
        "                        A[i, h, w, c] = np.sum(a_prev_slice)/(f*f)\n",
        "    \n",
        "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
        "    cache = (A_prev, hparameters)\n",
        "    # Making sure your output shape is correct\n",
        "    assert(A.shape == (m, n_H, n_W, n_C))\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "\n",
        "def conv_backward(dZ, cache):\n",
        "    #dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
        "    #cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "    #dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "    #           numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    #dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
        "    #      numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    #db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
        "    #      numpy array of shape (1, 1, 1, n_C)\n",
        "\n",
        "    (A_prev, W, b, hparameters) = cache\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "    stride = hparameters['stride']\n",
        "    pad = hparameters['pad']\n",
        "    (m, n_H, n_W, n_C) = dZ.shape\n",
        "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
        "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
        "    db = np.zeros((1, 1, 1, n_C))\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "    \n",
        "    for i in range(m):                      # loop over the training examples\n",
        "        \n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\n",
        "        a_prev_pad = A_prev_pad[i, :]\n",
        "        da_prev_pad = dA_prev_pad[i, :]\n",
        "        \n",
        "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):           # loop over the channels of the output volume\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\"\n",
        "                    vert_start = h*stride\n",
        "                    vert_end = vert_start+f\n",
        "                    horiz_start = w*stride\n",
        "                    horiz_end = horiz_start+f\n",
        "                    \n",
        "                    # Use the corners to define the slice from a_prev_pad\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
        "                    \n",
        "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
        "        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
        "    \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def create_mask_from_window(x):\n",
        "    #x -- Array of shape (f, f)\n",
        "    #mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
        "    mask = (x == np.max(x))\n",
        "    \n",
        "    return mask\n",
        "\n",
        "\n",
        "def distribute_value(dz, shape):\n",
        "    #dz -- input scalar\n",
        "    #shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
        "    #a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
        "    (n_H, n_W) = shape\n",
        "    average = n_H * n_W\n",
        "    a = np.ones(shape) * dz / average\n",
        "    \n",
        "    return a\n",
        "\n",
        "\n",
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "    #dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
        "    #cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
        "    #mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "    #dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
        "    (A_prev, hparameters) = cache\n",
        "    stride = hparameters[\"stride\"]\n",
        "    f = hparameters[\"f\"]\n",
        "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "    m, n_H, n_W, n_C = dA.shape\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "    \n",
        "    for i in range(m):                       # loop over the training examples\n",
        "        #select training example from A_prev (≈1 line)\n",
        "        a_prev = A_prev[i,:]\n",
        "        for h in range(n_H):                 #  loop on the vertical axis\n",
        "            for w in range(n_W):             #  loop on the horizontal axis\n",
        "                for c in range(n_C):         #  loop over the channels (depth)\n",
        "                    \n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f \n",
        "                    \n",
        "                    # Compute the backward propagation in both modes.\n",
        "                    if mode == \"max\":\n",
        "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
        "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
        "                        # Create the mask from a_prev_slice (≈1 line)\n",
        "                        mask = create_mask_from_window(a_prev_slice)\n",
        "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i,h,w,c]\n",
        "                        \n",
        "                    elif mode == \"average\":\n",
        "                        \n",
        "                        # Get the value a from dA (≈1 line)\n",
        "                        da = dA[i, h, w , c]\n",
        "                        # Define the shape of the filter as fxf (≈1 line)\n",
        "                        shape = (f,f)\n",
        "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
        "                        \n",
        "  \n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == A_prev.shape)\n",
        "    \n",
        "    return dA_prev\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(5, 5, 3, 2)\n",
        "hparameters = {\"stride\" : 1, \"f\": 2}\n",
        "A, cache = pool_forward(A_prev, hparameters)\n",
        "dA = np.random.randn(5, 4, 2, 2)\n",
        "\n",
        "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
        "print(\"mode = max\")\n",
        "print('mean of dA = ', np.mean(dA))\n",
        "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
        "print()\n",
        "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print('mean of dA = ', np.mean(dA))\n",
        "print('dA_prev[1,1] = ', dA_prev[1,1]) "
      ]
    }
  ]
}